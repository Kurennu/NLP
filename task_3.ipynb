{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать нейронную сеть с нуля, т.е. не используя готовые библиотеки. Пример работы на любом табличном датасете. \n",
    "Сделать класс, в котором реализована возможность задать количество нейронов в скрытом слое и провести обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет с данными о заболеваниях сердца, делим на обучающую и тестовую выборку, в качестве целевой переменной беру Disease (0 - нет патологии, 1 - есть). Так же выполнила one-hot encoding и нормализацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('heart_disease.csv')\n",
    "    df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "    X = df_encoded.drop('Disease', axis=1).values\n",
    "    y = df_encoded['Disease'].values.reshape(-1, 1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nРазмер обучающей выборки: {X_train.shape}\")\n",
    "    print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = X·W + b\n",
    "def linear_regression(X: np.ndarray, weights: np.ndarray, bias: float) -> np.ndarray:\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "# f(x) = 1 / (1 + exp(-x))\n",
    "def activation_function(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# производная\n",
    "def activation_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    fx = activation_function(x)\n",
    "    return fx * (1 - fx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuron:\n",
    "    def __init__(self, input_size: int, hidden_neurons: int = 0):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        scale = np.sqrt(2.0 / input_size)\n",
    "        \n",
    "        if hidden_neurons > 0:\n",
    "            self.W1 = np.random.randn(input_size, hidden_neurons) * scale\n",
    "            self.b1 = np.zeros((1, hidden_neurons))\n",
    "            self.W2 = np.random.randn(hidden_neurons, 1) * np.sqrt(2.0 / hidden_neurons)\n",
    "            self.b2 = 0.0\n",
    "        else:\n",
    "            self.weights = np.random.randn(input_size) * scale\n",
    "            self.bias = 0.0\n",
    "        \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.X = X\n",
    "        \n",
    "        if self.hidden_neurons > 0:\n",
    "            self.z1 = np.dot(X, self.W1) + self.b1\n",
    "            self.a1 = activation_function(self.z1)\n",
    "            \n",
    "            self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "            self.output = activation_function(self.z2)\n",
    "        else:\n",
    "            self.linear_output = np.dot(X, self.weights) + self.bias\n",
    "            self.output = activation_function(self.linear_output)\n",
    "        \n",
    "        if len(self.output.shape) == 1:\n",
    "            self.output = self.output.reshape(-1, 1)\n",
    "            \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, y_true: np.ndarray, learning_rate: float = 0.01) -> float:\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "\n",
    "        m = self.X.shape[0]\n",
    "\n",
    "        error = y_true - self.output\n",
    "        \n",
    "        if self.hidden_neurons > 0:\n",
    "            delta2 = error * activation_derivative(self.z2)\n",
    "            dW2 = np.dot(self.a1.T, delta2) / m\n",
    "            db2 = np.mean(delta2, axis=0)\n",
    "\n",
    "            delta1 = np.dot(delta2, self.W2.T) * activation_derivative(self.z1)\n",
    "            dW1 = np.dot(self.X.T, delta1) / m\n",
    "            db1 = np.mean(delta1, axis=0)\n",
    "\n",
    "            self.W2 += learning_rate * dW2\n",
    "            self.b2 += learning_rate * db2\n",
    "            self.W1 += learning_rate * dW1\n",
    "            self.b1 += learning_rate * db1.reshape(1, -1)\n",
    "        else:\n",
    "\n",
    "            delta = error * activation_derivative(self.linear_output)\n",
    "\n",
    "            dW = np.zeros_like(self.weights)\n",
    "            for i in range(m):\n",
    "                dW += self.X[i] * delta[i, 0]\n",
    "            dW /= m\n",
    "            \n",
    "            db = np.mean(delta)\n",
    "\n",
    "            self.weights += learning_rate * dW\n",
    "            self.bias += learning_rate * db\n",
    "\n",
    "        return np.mean(error ** 2)\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000, learning_rate: float = 0.01) -> list:\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.forward(X)\n",
    "            loss = self.backward(y, learning_rate)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, loss: {loss:.6f}\")\n",
    "                \n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_neuron(X_train, X_test, y_train, y_test, hidden_neurons=0):\n",
    "\n",
    "    print(f\"{hidden_neurons} нейронов в скрытом слое\\n\")\n",
    "\n",
    "    input_size = X_train.shape[1]\n",
    "    neuron = SimpleNeuron(input_size=input_size, hidden_neurons=hidden_neurons)\n",
    "\n",
    "    loss_history = neuron.train(X_train, y_train, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "    predictions = neuron.predict(X_test)\n",
    "    predicted_classes = (predictions > 0.5).astype(int)\n",
    "\n",
    "    accuracy = np.mean(predicted_classes == y_test)\n",
    "\n",
    "    tp = np.sum((predicted_classes == 1) & (y_test == 1))\n",
    "    tn = np.sum((predicted_classes == 0) & (y_test == 0))\n",
    "    fp = np.sum((predicted_classes == 1) & (y_test == 0))\n",
    "    fn = np.sum((predicted_classes == 0) & (y_test == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\\n\")\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(f\"Real {y_test[i][0]}\")\n",
    "        print(f\"Predictions {predictions[i][0]:.4f}\")\n",
    "        print(f\"Predicted {predicted_classes[i][0]}\\n\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Размер обучающей выборки: (216, 13)\n",
      "Размер тестовой выборки: (54, 13)\n",
      "50 нейронов в скрытом слое\n",
      "\n",
      "Epoch 0, loss: 0.343753\n",
      "Epoch 100, loss: 0.300684\n",
      "Epoch 200, loss: 0.274297\n",
      "Epoch 300, loss: 0.257428\n",
      "Epoch 400, loss: 0.244494\n",
      "Epoch 500, loss: 0.233454\n",
      "Epoch 600, loss: 0.223699\n",
      "Epoch 700, loss: 0.215016\n",
      "Epoch 800, loss: 0.207282\n",
      "Epoch 900, loss: 0.200392\n",
      "\n",
      "Accuracy: 0.8148\n",
      "Precision: 0.7895\n",
      "Recall: 0.7143\n",
      "F1-score: 0.7500\n",
      "\n",
      "Real 1\n",
      "Predictions 0.5818\n",
      "Predicted 1\n",
      "\n",
      "Real 1\n",
      "Predictions 0.4927\n",
      "Predicted 0\n",
      "\n",
      "Real 0\n",
      "Predictions 0.2818\n",
      "Predicted 0\n",
      "\n",
      "Real 0\n",
      "Predictions 0.4209\n",
      "Predicted 0\n",
      "\n",
      "Real 0\n",
      "Predictions 0.5085\n",
      "Predicted 1\n",
      "\n",
      "Real 1\n",
      "Predictions 0.4156\n",
      "Predicted 0\n",
      "\n",
      "Real 0\n",
      "Predictions 0.2646\n",
      "Predicted 0\n",
      "\n",
      "Real 0\n",
      "Predictions 0.3174\n",
      "Predicted 0\n",
      "\n",
      "Real 0\n",
      "Predictions 0.5583\n",
      "Predicted 1\n",
      "\n",
      "Real 0\n",
      "Predictions 0.4070\n",
      "Predicted 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, column_names = load_dataset()\n",
    "simple_neuron_metrics = test_simple_neuron(X_train, X_test, y_train, y_test, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать GPT как в п.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "attention_dim = 32\n",
    "seq_len = 64\n",
    "drop_rate = 0.1\n",
    "learning_rate = 0.01\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return text.split()\n",
    "\n",
    "def create_input_target_sequences(sequences):\n",
    "    input_sequences, target_sequences = [], []\n",
    "    for sentence in sequences:\n",
    "        mid = len(sentence) // 2\n",
    "        input_sequences.append(np.array(sentence[:mid]))\n",
    "        target_sequences.append(np.array(sentence[mid:]))\n",
    "    return input_sequences, target_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Он благополучно избегнул встречи с своею хозяйкой на лестнице. Каморка его приходилась под самою кровлей высокого пятиэтажного дома и походила более на шкаф, чем на квартиру. Квартирная же хозяйка его, у которой он нанимал эту каморку с обедом и прислугой, помещалась одною лестницей ниже, в отдельной квартире, и каждый раз, при выходе на улицу, ему непременно надо было проходить мимо хозяйкиной кухни, почти всегда настежь отворенной на лестницу. И каждый раз молодой человек, проходя мимо, чувствовал какое-то болезненное и трусливое ощущение, которого стыдился и от которого морщился. Он был должен кругом хозяйке и боялся с нею встретиться.\n",
    "\"\"\"\n",
    "\n",
    "sentences = text.strip().replace(\"\\n\", \" \").split(\". \")\n",
    "words = list(set(\" \".join(sentences).split())) \n",
    "words.sort()\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(words)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "sequences = [[word_to_index[word] for word in sentence.split()] for sentence in sentences]\n",
    "input_sequences, target_sequences = create_input_target_sequences(sequences)\n",
    "\n",
    "input_seqs_np = [seq.reshape(1, len(seq), 1) for seq in input_sequences]\n",
    "target_seqs_np = [seq.reshape(1, len(seq), 1) for seq in target_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTHead:\n",
    "    def __init__(self, embedding_dim, attention_dim, seq_len, drop_rate, vocab_size):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.drop_rate = drop_rate\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        init_scale = 0.02 \n",
    "        self.key_matrix = np.random.randn(embedding_dim, attention_dim) * init_scale\n",
    "        self.query_matrix = np.random.randn(embedding_dim, attention_dim) * init_scale\n",
    "        self.value_matrix = np.random.randn(embedding_dim, attention_dim) * init_scale\n",
    "        self.output_projection = np.random.randn(attention_dim, embedding_dim) * init_scale\n",
    "        \n",
    "        self.mask_matrix = np.tril(np.ones((seq_len, seq_len)))\n",
    "        self.embeddings = np.random.randn(vocab_size, embedding_dim) * init_scale\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "    def attention_forward(self, input_tensor, training=True):\n",
    "        B, T, C = input_tensor.shape\n",
    "        T_actual = min(T, self.seq_len)\n",
    "\n",
    "        key_proj = np.dot(input_tensor, self.key_matrix)\n",
    "        query_proj = np.dot(input_tensor, self.query_matrix)\n",
    "        value_proj = np.dot(input_tensor, self.value_matrix)\n",
    "        \n",
    "        scores = np.matmul(query_proj, key_proj.transpose(0, 2, 1)) * (self.attention_dim ** -0.5)\n",
    "        scores = np.where(self.mask_matrix[:T_actual, :T_actual] == 0, float('-inf'), scores)\n",
    "\n",
    "        scores_max = np.max(scores, axis=-1, keepdims=True)\n",
    "        exp_scores = np.exp(scores - scores_max)\n",
    "        attention_weights = exp_scores / (np.sum(exp_scores, axis=-1, keepdims=True) + 1e-10)\n",
    "\n",
    "        if training:\n",
    "            dropout_mask = (np.random.rand(*attention_weights.shape) > self.drop_rate) / (1 - self.drop_rate)\n",
    "\n",
    "        context = np.matmul(attention_weights, value_proj)\n",
    "        output_tensor = np.dot(context, self.output_projection)\n",
    "\n",
    "        self.cache = {\n",
    "            'key_proj': key_proj,\n",
    "            'query_proj': query_proj,\n",
    "            'value_proj': value_proj,\n",
    "            'attn_weights': attention_weights,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "        return output_tensor\n",
    "\n",
    "    def forward(self, input_tensor, training=True):\n",
    "        return self.attention_forward(input_tensor, training)\n",
    "\n",
    "    def calculate_loss(self, output_tensor, target_indices):\n",
    "        B, T, _ = output_tensor.shape\n",
    "        target_indices = target_indices.squeeze(axis=-1)\n",
    "        target_embeddings = np.zeros((B, T, self.embedding_dim))\n",
    "        \n",
    "        for b in range(B):\n",
    "            for t in range(T):\n",
    "                if t < len(target_indices[b]):\n",
    "                    target_embeddings[b, t] = self.embeddings[target_indices[b, t]]\n",
    "\n",
    "        loss = np.mean(np.sum((output_tensor - target_embeddings) ** 2, axis=-1))\n",
    "        return loss, target_embeddings\n",
    "\n",
    "    def train_step(self, input_tensor, target_tensor, learning_rate):\n",
    "        output_tensor = self.forward(input_tensor, training=True)\n",
    "\n",
    "        loss, target_embeddings = self.calculate_loss(output_tensor, target_tensor)\n",
    "        self.loss_history.append(loss)\n",
    "\n",
    "        grad_output = (output_tensor - target_embeddings) / (target_embeddings.size + 1e-10)\n",
    "        context = self.cache['context']\n",
    "\n",
    "        grad_output_projection = np.dot(context.reshape(-1, self.attention_dim).T,\n",
    "                                        grad_output.reshape(-1, self.embedding_dim))\n",
    "        \n",
    "        mean_grad = np.mean(grad_output, axis=0)\n",
    "        mean_input = np.mean(input_tensor, axis=0)\n",
    "        \n",
    "        grad_key = np.dot(mean_input.T, mean_grad)\n",
    "        grad_query = np.dot(mean_input.T, mean_grad)\n",
    "        grad_value = np.dot(mean_input.T, mean_grad)\n",
    "        \n",
    "        self.output_projection -= learning_rate * grad_output_projection\n",
    "        self.key_matrix -= learning_rate * grad_key[:, :self.attention_dim]\n",
    "        self.query_matrix -= learning_rate * grad_query[:, :self.attention_dim]\n",
    "        self.value_matrix -= learning_rate * grad_value[:, :self.attention_dim]\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def predict_next_words(self, input_tensor, num_words=3):\n",
    "        predictions = []\n",
    "        current_input = input_tensor.copy()\n",
    "        \n",
    "        for _ in range(num_words):\n",
    "            output = self.forward(current_input, training=False)\n",
    "            last_token_embedding = output[0, -1]\n",
    "\n",
    "            embeddings_norm = np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "            last_token_norm = np.linalg.norm(last_token_embedding)\n",
    "\n",
    "            if last_token_norm == 0 or np.any(embeddings_norm == 0):\n",
    "                similarities = np.zeros(self.vocab_size)\n",
    "            else:\n",
    "                cos_similarities = np.dot(self.embeddings, last_token_embedding) / (embeddings_norm * last_token_norm)\n",
    "                similarities = cos_similarities.flatten()\n",
    "\n",
    "            predicted_idx = np.argmax(similarities)\n",
    "            \n",
    "            if predicted_idx >= len(index_to_word) or predicted_idx < 0:\n",
    "                predicted_idx = 0\n",
    "                \n",
    "            predicted_word = index_to_word[predicted_idx]\n",
    "            predictions.append(predicted_word)\n",
    "                \n",
    "            return predictions\n",
    "    \n",
    "    def show_prediction_examples(self, batch_idx):\n",
    "        input_seq = input_seqs_np[batch_idx]\n",
    "        target_seq = target_seqs_np[batch_idx]\n",
    "\n",
    "        input_embeddings = np.zeros((1, input_seq.shape[1], self.embedding_dim))\n",
    "        for t in range(input_seq.shape[1]):\n",
    "            input_embeddings[0, t] = self.embeddings[input_seq[0, t, 0]]\n",
    "        \n",
    "        predictions = self.predict_next_words(input_embeddings, 3)\n",
    "        target_words = [index_to_word[target_seq[0, i, 0]] for i in range(min(3, target_seq.shape[1]))]\n",
    "        input_words = [index_to_word[input_seq[0, i, 0]] for i in range(min(5, input_seq.shape[1]))]\n",
    "        \n",
    "        print(\"\\nПример:\")\n",
    "        print(f\"Контекст: {' '.join(input_words)}...\")\n",
    "        print(f\"Предсказанное слово: {' '.join(predictions)}\")\n",
    "        print(f\"Реальные слова: {' '.join(target_words)}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Пример:\n",
      "Контекст: Он благополучно избегнул встречи...\n",
      "Предсказанное слово: И\n",
      "Реальные слова: с своею хозяйкой\n",
      "--------------------------------------------------\n",
      "\n",
      "Пример:\n",
      "Контекст: Каморка его приходилась под самою...\n",
      "Предсказанное слово: И\n",
      "Реальные слова: дома и походила\n",
      "--------------------------------------------------\n",
      "\n",
      "Пример:\n",
      "Контекст: Квартирная же хозяйка его, у...\n",
      "Предсказанное слово: И\n",
      "Реальные слова: и каждый раз,\n",
      "--------------------------------------------------\n",
      "\n",
      "Loss:\n",
      "Epoch 1: 0.05202415\n",
      "Epoch 2: 0.05202413\n",
      "Epoch 3: 0.05202410\n",
      "Epoch 4: 0.05202408\n",
      "Epoch 5: 0.05202406\n",
      "Epoch 6: 0.05202404\n",
      "Epoch 7: 0.05202401\n",
      "Epoch 8: 0.05202399\n",
      "Epoch 9: 0.05202397\n",
      "Epoch 10: 0.05202395\n",
      "Epoch 11: 0.05202392\n",
      "Epoch 12: 0.05202390\n",
      "Epoch 13: 0.05202388\n",
      "Epoch 14: 0.05202386\n",
      "Epoch 15: 0.05202383\n",
      "Epoch 16: 0.05202381\n",
      "Epoch 17: 0.05202379\n",
      "Epoch 18: 0.05202376\n",
      "Epoch 19: 0.05202374\n",
      "Epoch 20: 0.05202372\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = []\n",
    "for seq in input_seqs_np:\n",
    "    embedding_seq = np.zeros((1, seq.shape[1], embedding_dim))\n",
    "    for i in range(seq.shape[1]):\n",
    "        embedding_seq[0, i] = np.random.randn(embedding_dim) * 0.1 \n",
    "    input_embeddings.append(embedding_seq)\n",
    "\n",
    "gpt_head = GPTHead(embedding_dim, attention_dim, seq_len, drop_rate, vocab_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(len(input_embeddings)):\n",
    "        loss = gpt_head.train_step(input_embeddings[i], target_seqs_np[i], learning_rate)\n",
    "        total_loss += loss\n",
    "\n",
    "for i in range(min(3, len(input_embeddings))):\n",
    "    gpt_head.show_prediction_examples(i)\n",
    "\n",
    "print(\"\\nLoss:\")\n",
    "for i, loss in enumerate(gpt_head.loss_history[::len(input_embeddings)]):\n",
    "    print(f\"Epoch {i+1}: {loss:.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
