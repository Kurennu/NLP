{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1. Провести на любом тексте лемматизацию и стемминг (nltk, pymorphy2, pymorphy3, natasha)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy3\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В начале июля, в чрезвычайно жаркое время, под вечер, один молодой человек вышел из своей каморки, которую нанимал от жильцов в С — м переулке, на улицу и медленно, как бы в нерешимости, отправился к К — ну мосту. Он благополучно избегнул встречи с своею хозяйкой на лестнице. Каморка его приходилась под самою кровлей высокого пятиэтажного дома и походила более на шкаф, чем на квартиру. Квартирная же хозяйка его, у которой он нанимал эту каморку с обедом и прислугой, помещалась одною лестницей ниже, в отдельной квартире, и каждый раз, при выходе на улицу, ему непременно надо было проходить мимо хозяйкиной кухни, почти всегда настежь отворенной на лестницу. И каждый раз молодой человек, проходя мимо, чувствовал какое-то болезненное и трусливое ощущение, которого стыдился и от которого морщился. Он был должен кругом хозяйке и боялся с нею встретиться.\n"
     ]
    }
   ],
   "source": [
    "input_filename = 'text.txt'\n",
    "with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy3.MorphAnalyzer()\n",
    "stemmer = SnowballStemmer('russian')\n",
    "stopwords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яё\\s]', '', text)\n",
    "    words = text.split()\n",
    "\n",
    "    lemmatized = [morph.parse(word)[0].normal_form for word in words if word not in stopwords]\n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яё\\s]', '', text)\n",
    "    words = text.split()\n",
    "\n",
    "    stemmed = [stemmer.stem(word) for word in words if word not in stopwords]\n",
    "    return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "начало июль чрезвычайно жаркое время вечер молодой человек выйти свой каморка который нанимать жилец м переулок улица медленно нерешимость отправиться мост благополучно избегнуть встреча свой хозяйка лестница каморка приходиться самый кровля высокий пятиэтажный дом походить шкаф квартира квартирный хозяйка который нанимать каморка обед прислуга помещаться один лестница ниже отдельный квартира каждый выход улица непременно проходить мимо хозяйкин кухня настежь отворить лестница каждый молодой человек проходить мимо чувствовать какоеть болезненный трусливый ощущение который стыдиться который морщиться должный кругом хозяйка бояться она встретиться\n",
      "начал июл чрезвычайн жарк врем вечер молод человек вышел сво каморк котор нанима жильц м переулк улиц медлен нерешим отправ мост благополучн избегнул встреч сво хозяйк лестниц каморк приход сам кровл высок пятиэтажн дом поход шкаф квартир квартирн хозяйк котор нанима каморк обед прислуг помеща одн лестниц ниж отдельн квартир кажд выход улиц непремен проход мим хозяйкин кухн настеж отворен лестниц кажд молод человек проход мим чувствова какоет болезнен труслив ощущен котор стыд котор морщ долж круг хозяйк боя не встрет\n"
     ]
    }
   ],
   "source": [
    "lemmatized_text = lemmatize_text(text)\n",
    "stemmed_text = stem_text(text)\n",
    "\n",
    "print(lemmatized_text)\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Написать функцию для токенизации всех символов из ASCII</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    return [char for char in text if char != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['В', 'н', 'а', 'ч', 'а', 'л', 'е', 'и', 'ю', 'л', 'я', ',', 'в', 'ч', 'р', 'е', 'з', 'в', 'ы', 'ч', 'а', 'й', 'н', 'о', 'ж', 'а', 'р', 'к', 'о', 'е', 'в', 'р', 'е', 'м', 'я', ',', 'п', 'о', 'д', 'в', 'е', 'ч', 'е', 'р', ',', 'о', 'д', 'и', 'н', 'м', 'о', 'л', 'о', 'д', 'о', 'й', 'ч', 'е', 'л', 'о', 'в', 'е', 'к', 'в', 'ы', 'ш', 'е', 'л', 'и', 'з', 'с', 'в', 'о', 'е', 'й', 'к', 'а', 'м', 'о', 'р', 'к', 'и', ',', 'к', 'о', 'т', 'о', 'р', 'у', 'ю', 'н', 'а', 'н', 'и', 'м', 'а', 'л', 'о', 'т', 'ж', 'и', 'л', 'ь', 'ц', 'о', 'в', 'в', 'С', '—', 'м', 'п', 'е', 'р', 'е', 'у', 'л', 'к', 'е', ',', 'н', 'а', 'у', 'л', 'и', 'ц', 'у', 'и', 'м', 'е', 'д', 'л', 'е', 'н', 'н', 'о', ',', 'к', 'а', 'к', 'б', 'ы', 'в', 'н', 'е', 'р', 'е', 'ш', 'и', 'м', 'о', 'с', 'т', 'и', ',', 'о', 'т', 'п', 'р', 'а', 'в', 'и', 'л', 'с', 'я', 'к', 'К', '—', 'н', 'у', 'м', 'о', 'с', 'т', 'у', '.', 'О', 'н', 'б', 'л', 'а', 'г', 'о', 'п', 'о', 'л', 'у', 'ч', 'н', 'о', 'и', 'з', 'б', 'е', 'г', 'н', 'у', 'л', 'в', 'с', 'т', 'р', 'е', 'ч', 'и', 'с', 'с', 'в', 'о', 'е', 'ю', 'х', 'о', 'з', 'я', 'й', 'к', 'о', 'й', 'н', 'а', 'л', 'е', 'с', 'т', 'н', 'и', 'ц', 'е', '.', 'К', 'а', 'м', 'о', 'р', 'к', 'а', 'е', 'г', 'о', 'п', 'р', 'и', 'х', 'о', 'д', 'и', 'л', 'а', 'с', 'ь', 'п', 'о', 'д', 'с', 'а', 'м', 'о', 'ю', 'к', 'р', 'о', 'в', 'л', 'е', 'й', 'в', 'ы', 'с', 'о', 'к', 'о', 'г', 'о', 'п', 'я', 'т', 'и', 'э', 'т', 'а', 'ж', 'н', 'о', 'г', 'о', 'д', 'о', 'м', 'а', 'и', 'п', 'о', 'х', 'о', 'д', 'и', 'л', 'а', 'б', 'о', 'л', 'е', 'е', 'н', 'а', 'ш', 'к', 'а', 'ф', ',', 'ч', 'е', 'м', 'н', 'а', 'к', 'в', 'а', 'р', 'т', 'и', 'р', 'у', '.', 'К', 'в', 'а', 'р', 'т', 'и', 'р', 'н', 'а', 'я', 'ж', 'е', 'х', 'о', 'з', 'я', 'й', 'к', 'а', 'е', 'г', 'о', ',', 'у', 'к', 'о', 'т', 'о', 'р', 'о', 'й', 'о', 'н', 'н', 'а', 'н', 'и', 'м', 'а', 'л', 'э', 'т', 'у', 'к', 'а', 'м', 'о', 'р', 'к', 'у', 'с', 'о', 'б', 'е', 'д', 'о', 'м', 'и', 'п', 'р', 'и', 'с', 'л', 'у', 'г', 'о', 'й', ',', 'п', 'о', 'м', 'е', 'щ', 'а', 'л', 'а', 'с', 'ь', 'о', 'д', 'н', 'о', 'ю', 'л', 'е', 'с', 'т', 'н', 'и', 'ц', 'е', 'й', 'н', 'и', 'ж', 'е', ',', 'в', 'о', 'т', 'д', 'е', 'л', 'ь', 'н', 'о', 'й', 'к', 'в', 'а', 'р', 'т', 'и', 'р', 'е', ',', 'и', 'к', 'а', 'ж', 'д', 'ы', 'й', 'р', 'а', 'з', ',', 'п', 'р', 'и', 'в', 'ы', 'х', 'о', 'д', 'е', 'н', 'а', 'у', 'л', 'и', 'ц', 'у', ',', 'е', 'м', 'у', 'н', 'е', 'п', 'р', 'е', 'м', 'е', 'н', 'н', 'о', 'н', 'а', 'д', 'о', 'б', 'ы', 'л', 'о', 'п', 'р', 'о', 'х', 'о', 'д', 'и', 'т', 'ь', 'м', 'и', 'м', 'о', 'х', 'о', 'з', 'я', 'й', 'к', 'и', 'н', 'о', 'й', 'к', 'у', 'х', 'н', 'и', ',', 'п', 'о', 'ч', 'т', 'и', 'в', 'с', 'е', 'г', 'д', 'а', 'н', 'а', 'с', 'т', 'е', 'ж', 'ь', 'о', 'т', 'в', 'о', 'р', 'е', 'н', 'н', 'о', 'й', 'н', 'а', 'л', 'е', 'с', 'т', 'н', 'и', 'ц', 'у', '.', 'И', 'к', 'а', 'ж', 'д', 'ы', 'й', 'р', 'а', 'з', 'м', 'о', 'л', 'о', 'д', 'о', 'й', 'ч', 'е', 'л', 'о', 'в', 'е', 'к', ',', 'п', 'р', 'о', 'х', 'о', 'д', 'я', 'м', 'и', 'м', 'о', ',', 'ч', 'у', 'в', 'с', 'т', 'в', 'о', 'в', 'а', 'л', 'к', 'а', 'к', 'о', 'е', '-', 'т', 'о', 'б', 'о', 'л', 'е', 'з', 'н', 'е', 'н', 'н', 'о', 'е', 'и', 'т', 'р', 'у', 'с', 'л', 'и', 'в', 'о', 'е', 'о', 'щ', 'у', 'щ', 'е', 'н', 'и', 'е', ',', 'к', 'о', 'т', 'о', 'р', 'о', 'г', 'о', 'с', 'т', 'ы', 'д', 'и', 'л', 'с', 'я', 'и', 'о', 'т', 'к', 'о', 'т', 'о', 'р', 'о', 'г', 'о', 'м', 'о', 'р', 'щ', 'и', 'л', 'с', 'я', '.', 'О', 'н', 'б', 'ы', 'л', 'д', 'о', 'л', 'ж', 'е', 'н', 'к', 'р', 'у', 'г', 'о', 'м', 'х', 'о', 'з', 'я', 'й', 'к', 'е', 'и', 'б', 'о', 'я', 'л', 'с', 'я', 'с', 'н', 'е', 'ю', 'в', 'с', 'т', 'р', 'е', 'т', 'и', 'т', 'ь', 'с', 'я', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3. Написать функцию для векторизации всех символов из ASCII</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1042, 1085, 1072, 1095, 1072, 1083, 1077, 1080, 1102, 1083, 1103, 44, 1074, 1095, 1088, 1077, 1079, 1074, 1099, 1095, 1072, 1081, 1085, 1086, 1078, 1072, 1088, 1082, 1086, 1077, 1074, 1088, 1077, 1084, 1103, 44, 1087, 1086, 1076, 1074, 1077, 1095, 1077, 1088, 44, 1086, 1076, 1080, 1085, 1084, 1086, 1083, 1086, 1076, 1086, 1081, 1095, 1077, 1083, 1086, 1074, 1077, 1082, 1074, 1099, 1096, 1077, 1083, 1080, 1079, 1089, 1074, 1086, 1077, 1081, 1082, 1072, 1084, 1086, 1088, 1082, 1080, 44, 1082, 1086, 1090, 1086, 1088, 1091, 1102, 1085, 1072, 1085, 1080, 1084, 1072, 1083, 1086, 1090, 1078, 1080, 1083, 1100, 1094, 1086, 1074, 1074, 1057, 8212, 1084, 1087, 1077, 1088, 1077, 1091, 1083, 1082, 1077, 44, 1085, 1072, 1091, 1083, 1080, 1094, 1091, 1080, 1084, 1077, 1076, 1083, 1077, 1085, 1085, 1086, 44, 1082, 1072, 1082, 1073, 1099, 1074, 1085, 1077, 1088, 1077, 1096, 1080, 1084, 1086, 1089, 1090, 1080, 44, 1086, 1090, 1087, 1088, 1072, 1074, 1080, 1083, 1089, 1103, 1082, 1050, 8212, 1085, 1091, 1084, 1086, 1089, 1090, 1091, 46, 1054, 1085, 1073, 1083, 1072, 1075, 1086, 1087, 1086, 1083, 1091, 1095, 1085, 1086, 1080, 1079, 1073, 1077, 1075, 1085, 1091, 1083, 1074, 1089, 1090, 1088, 1077, 1095, 1080, 1089, 1089, 1074, 1086, 1077, 1102, 1093, 1086, 1079, 1103, 1081, 1082, 1086, 1081, 1085, 1072, 1083, 1077, 1089, 1090, 1085, 1080, 1094, 1077, 46, 1050, 1072, 1084, 1086, 1088, 1082, 1072, 1077, 1075, 1086, 1087, 1088, 1080, 1093, 1086, 1076, 1080, 1083, 1072, 1089, 1100, 1087, 1086, 1076, 1089, 1072, 1084, 1086, 1102, 1082, 1088, 1086, 1074, 1083, 1077, 1081, 1074, 1099, 1089, 1086, 1082, 1086, 1075, 1086, 1087, 1103, 1090, 1080, 1101, 1090, 1072, 1078, 1085, 1086, 1075, 1086, 1076, 1086, 1084, 1072, 1080, 1087, 1086, 1093, 1086, 1076, 1080, 1083, 1072, 1073, 1086, 1083, 1077, 1077, 1085, 1072, 1096, 1082, 1072, 1092, 44, 1095, 1077, 1084, 1085, 1072, 1082, 1074, 1072, 1088, 1090, 1080, 1088, 1091, 46, 1050, 1074, 1072, 1088, 1090, 1080, 1088, 1085, 1072, 1103, 1078, 1077, 1093, 1086, 1079, 1103, 1081, 1082, 1072, 1077, 1075, 1086, 44, 1091, 1082, 1086, 1090, 1086, 1088, 1086, 1081, 1086, 1085, 1085, 1072, 1085, 1080, 1084, 1072, 1083, 1101, 1090, 1091, 1082, 1072, 1084, 1086, 1088, 1082, 1091, 1089, 1086, 1073, 1077, 1076, 1086, 1084, 1080, 1087, 1088, 1080, 1089, 1083, 1091, 1075, 1086, 1081, 44, 1087, 1086, 1084, 1077, 1097, 1072, 1083, 1072, 1089, 1100, 1086, 1076, 1085, 1086, 1102, 1083, 1077, 1089, 1090, 1085, 1080, 1094, 1077, 1081, 1085, 1080, 1078, 1077, 44, 1074, 1086, 1090, 1076, 1077, 1083, 1100, 1085, 1086, 1081, 1082, 1074, 1072, 1088, 1090, 1080, 1088, 1077, 44, 1080, 1082, 1072, 1078, 1076, 1099, 1081, 1088, 1072, 1079, 44, 1087, 1088, 1080, 1074, 1099, 1093, 1086, 1076, 1077, 1085, 1072, 1091, 1083, 1080, 1094, 1091, 44, 1077, 1084, 1091, 1085, 1077, 1087, 1088, 1077, 1084, 1077, 1085, 1085, 1086, 1085, 1072, 1076, 1086, 1073, 1099, 1083, 1086, 1087, 1088, 1086, 1093, 1086, 1076, 1080, 1090, 1100, 1084, 1080, 1084, 1086, 1093, 1086, 1079, 1103, 1081, 1082, 1080, 1085, 1086, 1081, 1082, 1091, 1093, 1085, 1080, 44, 1087, 1086, 1095, 1090, 1080, 1074, 1089, 1077, 1075, 1076, 1072, 1085, 1072, 1089, 1090, 1077, 1078, 1100, 1086, 1090, 1074, 1086, 1088, 1077, 1085, 1085, 1086, 1081, 1085, 1072, 1083, 1077, 1089, 1090, 1085, 1080, 1094, 1091, 46, 1048, 1082, 1072, 1078, 1076, 1099, 1081, 1088, 1072, 1079, 1084, 1086, 1083, 1086, 1076, 1086, 1081, 1095, 1077, 1083, 1086, 1074, 1077, 1082, 44, 1087, 1088, 1086, 1093, 1086, 1076, 1103, 1084, 1080, 1084, 1086, 44, 1095, 1091, 1074, 1089, 1090, 1074, 1086, 1074, 1072, 1083, 1082, 1072, 1082, 1086, 1077, 45, 1090, 1086, 1073, 1086, 1083, 1077, 1079, 1085, 1077, 1085, 1085, 1086, 1077, 1080, 1090, 1088, 1091, 1089, 1083, 1080, 1074, 1086, 1077, 1086, 1097, 1091, 1097, 1077, 1085, 1080, 1077, 44, 1082, 1086, 1090, 1086, 1088, 1086, 1075, 1086, 1089, 1090, 1099, 1076, 1080, 1083, 1089, 1103, 1080, 1086, 1090, 1082, 1086, 1090, 1086, 1088, 1086, 1075, 1086, 1084, 1086, 1088, 1097, 1080, 1083, 1089, 1103, 46, 1054, 1085, 1073, 1099, 1083, 1076, 1086, 1083, 1078, 1077, 1085, 1082, 1088, 1091, 1075, 1086, 1084, 1093, 1086, 1079, 1103, 1081, 1082, 1077, 1080, 1073, 1086, 1103, 1083, 1089, 1103, 1089, 1085, 1077, 1102, 1074, 1089, 1090, 1088, 1077, 1090, 1080, 1090, 1100, 1089, 1103, 46]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens: list[str]) -> list[int]:\n",
    "    return [ord(char) for char in tokens]\n",
    "\n",
    "vectorized_text = vectorize(tokenized_text)\n",
    "print(vectorized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
